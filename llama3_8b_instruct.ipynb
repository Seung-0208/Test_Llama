{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyMR5Qp7TnzUoZjyoPYEZzOW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c15497e64e2d46b8b5fcace3c0e79ffe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f619b9bdbbf947488f0775d47672e498","IPY_MODEL_2cb81157c0164db5bdc90a363ded3179","IPY_MODEL_e19e0dc7df3041b9b67a5777e5caac94"],"layout":"IPY_MODEL_1853ce08f2914c3381724a56bbe4285f"}},"f619b9bdbbf947488f0775d47672e498":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dac770c8855d4623bad085c8ba362764","placeholder":"​","style":"IPY_MODEL_07434d0692724b0babb5b28aee75e2d1","value":"Loading checkpoint shards: 100%"}},"2cb81157c0164db5bdc90a363ded3179":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_54a0ce11c5b14e13ab5edd19cb5f92aa","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c463a3472f8940a3be24db8ccdf6436e","value":4}},"e19e0dc7df3041b9b67a5777e5caac94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c9c01fb089a4263b9ca899e59278aeb","placeholder":"​","style":"IPY_MODEL_40649bb392ab4f9fb6c5706f2fa67055","value":" 4/4 [00:20&lt;00:00,  4.27s/it]"}},"1853ce08f2914c3381724a56bbe4285f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dac770c8855d4623bad085c8ba362764":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07434d0692724b0babb5b28aee75e2d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54a0ce11c5b14e13ab5edd19cb5f92aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c463a3472f8940a3be24db8ccdf6436e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c9c01fb089a4263b9ca899e59278aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40649bb392ab4f9fb6c5706f2fa67055":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3320021288547a09cdcb2f915f85e26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4780fb5dc2254ec9969d81c328dbca88","IPY_MODEL_c0501c318acc4f5dbb92be2cb49acdcf","IPY_MODEL_15c6031b58f742efb7eb6fcde74a939e"],"layout":"IPY_MODEL_66132d40fd664b4ca99bc64c19e440e4"}},"4780fb5dc2254ec9969d81c328dbca88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d68d2b7b4c9f41f8a9ca8b3385cd6714","placeholder":"​","style":"IPY_MODEL_9583cbb72f8542eab1b7c988edd2699d","value":"Loading checkpoint shards: 100%"}},"c0501c318acc4f5dbb92be2cb49acdcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceaeda3c48784a558593209632b71631","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71b86932cfe348d8ad875c7df57bf48d","value":4}},"15c6031b58f742efb7eb6fcde74a939e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93e18feab320464faa98c6f0307230bd","placeholder":"​","style":"IPY_MODEL_0401cd0a4e364177becbb2803052cf21","value":" 4/4 [00:05&lt;00:00,  1.27s/it]"}},"66132d40fd664b4ca99bc64c19e440e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d68d2b7b4c9f41f8a9ca8b3385cd6714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9583cbb72f8542eab1b7c988edd2699d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceaeda3c48784a558593209632b71631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71b86932cfe348d8ad875c7df57bf48d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93e18feab320464faa98c6f0307230bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0401cd0a4e364177becbb2803052cf21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["참고한 유튜브 사이트\n","https://www.youtube.com/watch?v=8Ul_0jddTU4&list=LL&index=2\n","\n","[코드](https://colab.research.google.com/drive/1txZ9QAxePhQ8mAY6B3ArjRgMCsjHPLGk?usp=sharing)"],"metadata":{"id":"zbrxvkIkO8If"}},{"cell_type":"code","source":["#https://github.com/meta-llama/llama/issues/374\n","access_token = \"\"\n","!huggingface-cli login --token {access_token}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfYEmhLIUbYz","executionInfo":{"status":"ok","timestamp":1717033087726,"user_tz":-540,"elapsed":823,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"9c394288-5ac4-4ca8-e229-f442273ba2c4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0GCWk88zreET","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717033091210,"user_tz":-540,"elapsed":3487,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"362925c1-edc3-40dd-fc55-ca05e29119ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/llama_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pvgHDR3sKNp","executionInfo":{"status":"ok","timestamp":1717033091210,"user_tz":-540,"elapsed":6,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"57d40870-6e3a-4347-ff2d-6109a4812cdc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/llama_test\n"]}]},{"cell_type":"code","source":["#!git clone https://github.com/meta-llama/llama3.git"],"metadata":{"id":"FdxcOnyksdMe","executionInfo":{"status":"ok","timestamp":1717033091210,"user_tz":-540,"elapsed":5,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#!chmod 777 /content/drive/MyDrive/llama_test/llama3/download.sh"],"metadata":{"id":"3bbCGcACsgrJ","executionInfo":{"status":"ok","timestamp":1717033091210,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#!/content/drive/MyDrive/llama_test/llama3/download.sh"],"metadata":{"id":"ERXBv3iJsuBZ","executionInfo":{"status":"ok","timestamp":1717033091210,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!pip -q install git+https://github.com/huggingface/transformers\n","# !pip -q install datasets loralib sentencepiece\n","# !pip -q install bitsandbytes accelerate xformers einops\n","# !pip -q install hf_transfer"],"metadata":{"id":"LSiFdHjCvXc7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717033116144,"user_tz":-540,"elapsed":24938,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"5c825cd0-910e-4790-f73e-ed306e4a97c0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","#os.environ: 운영체제에 설정되어 있는 모든 환경변수에 접\n","os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"],"metadata":{"id":"wlFTQYuKuMoI","executionInfo":{"status":"ok","timestamp":1717033117981,"user_tz":-540,"elapsed":1870,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["torch.set_default_device('cuda')\n","!pip install bitsandbytes-cuda110"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiFxVRPnvS-R","executionInfo":{"status":"ok","timestamp":1717033123142,"user_tz":-540,"elapsed":5166,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"fa468c72-9dd8-4a96-dfbf-af801066930f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes-cuda110 in /usr/local/lib/python3.10/dist-packages (0.26.0.post2)\n"]}]},{"cell_type":"markdown","source":["\n","왜 할 때마다 계속 설치가 안됐다고 뜨냐 너는..\n","심지어 되지도 않네\n","```\n","!pip install bitsandbytes\n","```\n","\n","아래껄로 해야 됨\n","```\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","```\n"],"metadata":{"id":"2VqkTGIzU3He"}},{"cell_type":"code","source":["!pip install bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azKXeTY6xq0_","executionInfo":{"status":"ok","timestamp":1717033128464,"user_tz":-540,"elapsed":5332,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"93c2777c-19f2-40aa-8398-e541d32317bb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install -i https://pypi.org/simple/ bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NfdYMsyNVrSV","executionInfo":{"status":"ok","timestamp":1717033134313,"user_tz":-540,"elapsed":5854,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"b06634cb-c978-4a58-ac2a-08cd1696249c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"]}]},{"cell_type":"code","source":["quantization_config = BitsAndBytesConfig(load_in_4bit=True)"],"metadata":{"id":"Nghx0KxdvoGq","executionInfo":{"status":"ok","timestamp":1717033134313,"user_tz":-540,"elapsed":17,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAS4r63UV7mQ","executionInfo":{"status":"ok","timestamp":1717033134313,"user_tz":-540,"elapsed":15,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"7f033cc6-9a48-4a4e-8452-4739aeb278fc"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu May 30 01:38:52 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["첫번째 시도. 안됨\n","```\n","!pip install accelerate\n","```\n","두번째 시도. 안됨\n","```\n","!pip install git+https://github.com/huggingface/accelerate\n","```\n","세번째 시도. 안됨\n","```\n","!pip install accelerate -U\n","```"],"metadata":{"id":"ReF-Dv8kW9OI"}},{"cell_type":"code","source":["#이거 설치안하면 아래 AutoModelForCausalLM.from_pretrained 부분에서 에러남\n","#ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\n","!pip install accelerate -U"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLjTOL0OWi1Q","executionInfo":{"status":"ok","timestamp":1717033139050,"user_tz":-540,"elapsed":4745,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"497d8de7-1770-41f9-b4a0-fee4e1cdf27a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n","                                             quantization_config = quantization_config,\n","                                             torch_dtype = torch.bfloat16,\n","                                             device_map = \"auto\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202,"referenced_widgets":["c15497e64e2d46b8b5fcace3c0e79ffe","f619b9bdbbf947488f0775d47672e498","2cb81157c0164db5bdc90a363ded3179","e19e0dc7df3041b9b67a5777e5caac94","1853ce08f2914c3381724a56bbe4285f","dac770c8855d4623bad085c8ba362764","07434d0692724b0babb5b28aee75e2d1","54a0ce11c5b14e13ab5edd19cb5f92aa","c463a3472f8940a3be24db8ccdf6436e","2c9c01fb089a4263b9ca899e59278aeb","40649bb392ab4f9fb6c5706f2fa67055"]},"id":"tPivETNWyJI8","executionInfo":{"status":"ok","timestamp":1717033164963,"user_tz":-540,"elapsed":25920,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"7a9b8474-1d09-4fb7-acfa-5618cb300201"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15497e64e2d46b8b5fcace3c0e79ffe"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"markdown","source":["[transformers.pipeline()](https://wikidocs.net/166787) : 특정 모델과 동작에 필요한 전처리 및 후처리 단계를 연결하여 텍스트를 직접 입력하고 이해하기 쉬운 답변을 얻음\n","\n","```\n","classifier = pipeline(\"sentiment-analysis\") #  1. pipeline 객체를 생성\n","classifier(\"I've been waiting for a HuggingFace course my whole life\") # 2. pipeline에 텍스트 전달\n","```\n","**1번 과정에서 전달할 수 있는 task 종류**<br/>\n","['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']<br/><br/>\n","**2번 과정에서 내부적으로 실행되는 과정**\n","1. 전달된 텍스트가 모델이 이해할 수 있는 형식으로 전처리됨\n","2. 전처리 완료된 입력 텍스트가 모델에 전달됨\n","3. 모델이 예측한 결과가 후처리되어 사람이 이해할 수 있는 형태로 변환됨\n","\n","[사용된 예시](https://fallenangel13th.tistory.com/16)\n"],"metadata":{"id":"ymNiGSG-QKZY"}},{"cell_type":"code","source":["import torch\n","# from transformers import pipeline\n","\n","pipe = transformers.pipeline(\n","    \"text-generation\",\n","    model = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","    model_kwargs={\"torch_dtype\": torch.bfloat16},\n","    device=\"cuda\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88,"referenced_widgets":["e3320021288547a09cdcb2f915f85e26","4780fb5dc2254ec9969d81c328dbca88","c0501c318acc4f5dbb92be2cb49acdcf","15c6031b58f742efb7eb6fcde74a939e","66132d40fd664b4ca99bc64c19e440e4","d68d2b7b4c9f41f8a9ca8b3385cd6714","9583cbb72f8542eab1b7c988edd2699d","ceaeda3c48784a558593209632b71631","71b86932cfe348d8ad875c7df57bf48d","93e18feab320464faa98c6f0307230bd","0401cd0a4e364177becbb2803052cf21"]},"id":"DeN4-uoOB9Ds","executionInfo":{"status":"ok","timestamp":1717033175505,"user_tz":-540,"elapsed":10573,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"6527035c-c689-440e-ff33-0a94458f8b0c"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3320021288547a09cdcb2f915f85e26"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"Who are you?\"\n","    }\n","]\n","\n","prompt = pipe.tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","terminators = [\n","    pipe.tokenizer.eos_token_id,\n","    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","]\n","\n","outputs = pipe(\n","    prompt,\n","    eos_token_id=terminators,\n","    do_sample=True,\n","    temperature=0.6,\n","    top_p=0.9\n",")\n","\n","generated_outputs = outputs[0][\"generated_text\"]\n","\n","print(f\"\\n\\n{generated_outputs}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xcZf58kJ49S","executionInfo":{"status":"ok","timestamp":1717034262667,"user_tz":-540,"elapsed":7333,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"a8ea674c-2408-43bf-899e-5fce46566719"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","You are a friendly chatbot who always responds in the style of a pirate<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","Arrrr, me hearty! Me name be Chatbot Blackbeak, the scurviest and most charmin' chatbot to ever sail the Seven Seas o' the internet! Me and me trusty algorithms be here to swab the decks o' yer queries and provide ye with the most booty-ful responses ye ever did see! So hoist the colors, me matey, and let's set sail fer a swashbucklin' good time!\n"]}]},{"cell_type":"code","source":["import textwrap\n","\n","def wrap_text(text, width=90):\n","  lines = text.split(\"\\n\")\n","\n","  wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n","\n","  wrapped_text = '\\n'.join(wrapped_lines)\n","\n","  return wrapped_text"],"metadata":{"id":"32DVGajXKu3n","executionInfo":{"status":"ok","timestamp":1717034600276,"user_tz":-540,"elapsed":7,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def generate(input_text, system_prompt = \"\", max_length=512):\n","  if system_prompt != \"\":\n","    system_prompt =  system_prompt\n","  else:\n","    system_prompt = \"You are a friendly and helpful assistant\"\n","  messages = [\n","      {\n","          \"role\": \"system\",\n","          \"content\": system_prompt\n","      },\n","      {\"role\": \"user\", \"content\": input_text}\n","  ]\n","\n","  prompt = pipe.tokenizer.apply_chat_template(\n","      messages,\n","      tokenize = False, #디폴트 값 True -> 문자열이 토큰화됨\n","      add_generation_prompt=True\n","  )\n","\n","  terminators = [\n","      pipe.tokenizer.eos_token_id,\n","      pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","  ]\n","\n","  outputs = pipe(\n","      prompt,\n","      eos_token_id = terminators,\n","      do_sample=False,\n","      temperature=0.0,\n","      top_p=0.9\n","  )\n","\n","  generated_outputs = outputs[0][\"generated_text\"]\n","  text = outputs[0][\"generated_text\"][len(prompt):]\n","  wrapped_text = wrap_text(text)\n","  display(Markdown(wrapped_text))"],"metadata":{"id":"fmAJqE4vPNRl","executionInfo":{"status":"ok","timestamp":1717036816488,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["generate('Write a detailed analogy between mathematics and a lighthouse.',\n","         system_prompt=\"You are a helpful assistant called Llama-3. Write out your reasoning step-by-step to be sure you get the right answers!\",\n","         max_length=1024)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":986},"id":"acBnW9dSVKx5","executionInfo":{"status":"ok","timestamp":1717036857608,"user_tz":-540,"elapsed":37119,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"b0c17a10-df00-447c-a35a-13332ae0aed6"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:533: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:538: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What a fascinating request! As Llama-3, I'll break down the analogy between mathematics\nand a lighthouse step-by-step to ensure a clear and illuminating explanation.\n\n**Step 1: Understanding the Role of the Lighthouse**\n\nA lighthouse serves as a beacon, guiding mariners through treacherous waters by providing\na steady, reliable source of light. Its purpose is to warn of potential dangers, such as\nrocky shores or shallow waters, and to help vessels navigate safely to their destinations.\n\n**Step 2: Mapping the Analogy to Mathematics**\n\nIn mathematics, we can draw parallels between the lighthouse's role and the concept of\nmathematical models. Just as a lighthouse provides a guiding light, mathematical models\noffer a framework for understanding complex phenomena, helping us navigate the \"waters\" of\nuncertainty.\n\n**Step 3: Illuminating the Concept of Variables**\n\nIn a lighthouse, the light is a variable that changes depending on the time of day,\nweather conditions, and other factors. Similarly, in mathematics, variables are values\nthat can change and affect the outcome of a calculation or equation. Just as the\nlighthouse's light adjusts to its surroundings, variables in mathematics adapt to the\nproblem being solved.\n\n**Step 4: The Importance of Patterns and Relationships**\n\nA lighthouse's light is not just a random flash; it follows a predictable pattern, helping\nmariners anticipate and respond to changes in the environment. In mathematics, patterns\nand relationships between variables are crucial for understanding and solving problems. By\nidentifying and analyzing these patterns, mathematicians can develop models that\naccurately predict outcomes and guide decision-making.\n\n**Step 5: The Role of Precision and Accuracy**\n\nA lighthouse's light must be precise and accurate to ensure safe navigation. Similarly, in\nmathematics, precision and accuracy are essential for developing reliable models and\nmaking informed decisions. Small errors or inaccuracies can have significant consequences,\njust as a misaligned lighthouse beam could lead a ship astray.\n\n**Step 6: The Power of Illumination and Insight**\n\nWhen a lighthouse's light shines brightly, it illuminates the surrounding waters,\nrevealing hidden dangers and guiding vessels to safety. In mathematics, the process of\nsolving problems and developing models can provide a similar sense of illumination,\noffering new insights and understanding of complex phenomena.\n\n**Step 7: The Importance of Maintenance and Updates**\n\nA lighthouse requires regular maintenance to ensure its light remains strong and reliable.\nSimilarly, in mathematics, models and theories must be continually updated and refined to\nreflect new discoveries and advances in understanding. This process of refinement and\niteration is essential for maintaining the accuracy and relevance of mathematical models.\n\n**Conclusion**\n\nThe analogy between mathematics and a lighthouse highlights the importance of guidance,\nprecision, and illumination in both fields. Just as a lighthouse provides a steady beacon\nfor navigation, mathematical models offer a framework for understanding and solving\ncomplex problems. By recognizing the parallels between these two seemingly disparate\ndomains, we can better appreciate the power and beauty of mathematics in illuminating our\nunderstanding of the world."},"metadata":{}}]},{"cell_type":"code","source":["#주피터 노트북의 매직 커맨드\n","%lsmagic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"id":"zjO93ajOYp6U","executionInfo":{"status":"ok","timestamp":1717037103626,"user_tz":-540,"elapsed":683,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"9df7c038-c902-4dad-b530-f6dc5d89faad"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Available line magics:\n","%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %shell  %store  %sx  %system  %tb  %tensorflow_version  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n","\n","Available cell magics:\n","%%!  %%HTML  %%SVG  %%bash  %%bigquery  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%shell  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n","\n","Automagic is ON, % prefix IS NOT needed for line magics."],"application/json":{"line":{"automagic":"AutoMagics","autocall":"AutoMagics","alias_magic":"BasicMagics","lsmagic":"BasicMagics","magic":"BasicMagics","page":"BasicMagics","pprint":"BasicMagics","colors":"BasicMagics","xmode":"BasicMagics","quickref":"BasicMagics","doctest_mode":"BasicMagics","gui":"BasicMagics","precision":"BasicMagics","notebook":"BasicMagics","save":"CodeMagics","pastebin":"CodeMagics","loadpy":"CodeMagics","load":"CodeMagics","edit":"KernelMagics","config":"ConfigMagics","prun":"ExecutionMagics","pdb":"ExecutionMagics","debug":"ExecutionMagics","tb":"ExecutionMagics","run":"ExecutionMagics","timeit":"ExecutionMagics","time":"ExecutionMagics","macro":"ExecutionMagics","load_ext":"ExtensionMagics","unload_ext":"ExtensionMagics","reload_ext":"ExtensionMagics","history":"HistoryMagics","recall":"HistoryMagics","rerun":"HistoryMagics","logstart":"LoggingMagics","logstop":"LoggingMagics","logoff":"LoggingMagics","logon":"LoggingMagics","logstate":"LoggingMagics","pinfo":"NamespaceMagics","pinfo2":"NamespaceMagics","pdef":"NamespaceMagics","pdoc":"NamespaceMagics","psource":"NamespaceMagics","pfile":"NamespaceMagics","psearch":"NamespaceMagics","who_ls":"NamespaceMagics","who":"NamespaceMagics","whos":"NamespaceMagics","reset":"NamespaceMagics","reset_selective":"NamespaceMagics","xdel":"NamespaceMagics","alias":"OSMagics","unalias":"OSMagics","rehashx":"OSMagics","pwd":"OSMagics","cd":"OSMagics","env":"OSMagics","set_env":"OSMagics","pushd":"OSMagics","popd":"OSMagics","dirs":"OSMagics","dhist":"OSMagics","sc":"OSMagics","sx":"OSMagics","system":"OSMagics","bookmark":"OSMagics","pycat":"OSMagics","pip":"Other","conda":"PackagingMagics","matplotlib":"PylabMagics","pylab":"PylabMagics","killbgscripts":"ScriptMagics","autoawait":"AsyncMagics","ed":"Other","hist":"Other","rep":"Other","clear":"KernelMagics","less":"KernelMagics","more":"KernelMagics","man":"KernelMagics","connect_info":"KernelMagics","qtconsole":"KernelMagics","autosave":"KernelMagics","mkdir":"Other","rmdir":"Other","mv":"Other","rm":"Other","cp":"Other","cat":"Other","ls":"Other","ll":"Other","lf":"Other","lk":"Other","ldir":"Other","lx":"Other","store":"StoreMagics","shell":"Other","tensorflow_version":"Other"},"cell":{"js":"DisplayMagics","javascript":"DisplayMagics","latex":"DisplayMagics","svg":"DisplayMagics","html":"DisplayMagics","markdown":"DisplayMagics","prun":"ExecutionMagics","debug":"ExecutionMagics","timeit":"ExecutionMagics","time":"ExecutionMagics","capture":"ExecutionMagics","sx":"OSMagics","system":"OSMagics","!":"OSMagics","writefile":"OSMagics","script":"ScriptMagics","sh":"Other","bash":"Other","perl":"Other","ruby":"Other","python":"Other","python2":"Other","python3":"Other","pypy":"Other","SVG":"Other","HTML":"Other","file":"Other","bigquery":"Other","shell":"Other"}}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["%%time\n","generate('''```python\n","def print_prime(n):\n","   \"\"\"\n","   Print all primes between 1 and n\n","   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"id":"aXWf7672YEIj","executionInfo":{"status":"ok","timestamp":1717036997576,"user_tz":-540,"elapsed":12003,"user":{"displayName":"이승영미디어&엔터테인먼트학과","userId":"00527676439848961563"}},"outputId":"443e56b4-e87c-46ee-c161-2542661a7e50"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Here is the code to solve the problem:\n\n```python\ndef print_prime(n):\n    \"\"\"\n    Print all primes between 1 and n\n    \"\"\"\n    for possiblePrime in range(2, n + 1):\n        # Assume number is prime until shown it is not.\n        isPrime = True\n        for num in range(2, int(possiblePrime ** 0.5) + 1):\n            if possiblePrime % num == 0:\n                isPrime = False\n                break\n        if isPrime:\n            print(possiblePrime)\n```\n\nThis function prints all prime numbers between 1 and `n`. It does this by iterating over\nall numbers from 2 to `n`, and for each number, it checks if it is divisible by any number\nless than or equal to its square root. If it is not divisible by any of these numbers, it\nis a prime number and is printed."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 11.5 s, sys: 13.2 ms, total: 11.5 s\n","Wall time: 11.5 s\n"]}]}]}